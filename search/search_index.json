{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":"<p>The api-syncagent is a Kubernetes agent responsible for integrating external Kubernetes clusters. It runs on a Kubernetes cluster, is configured with credentials to a kcp instance and will then synchronize data out of kcp (i.e. out of kcp workspaces) onto the local cluster, and vice versa.</p>"},{"location":"#high-level-overview","title":"High-level Overview","text":"<p>The intended usecase follows roughly these steps:</p> <ol> <li>A user in kcp with sufficient permissions creates an <code>APIExport</code> object and provides appropriate    credentials for the Sync Agent (e.g. by creating a Kubernetes Secret with a preconfigured kubeconfig    in it).</li> <li>A service owner will now take these credentials and the configured API group (the <code>APIExport</code>'s    name) and use them to setup the Sync Agent. It is assumed that the service owner (i.e. the    cluster-admin in a service cluster) wants to make some resources (usually CRDs) available to use    inside of kcp.</li> <li>The service owner uses the Sync Agent Helm chart (or similar deployment technique) to install the    Sync Agent in their cluster.</li> <li>To actually make resources available in kcp, the service owner now has to create a set of    <code>PublishedResource</code> objects. The configuration happens from their point of view, meaning they    define how to publish a CRD to kcp, defining renaming rules and other projection settings.</li> <li>Once a <code>PublishedResource</code> is created in the service cluster, the Sync Agent will pick it up,    find the referenced CRD, convert/project this CRD into an <code>APIResourceSchema</code> (ARS) for kcp and    then create the ARS in org workspace.</li> <li>Finally the Sync Agent will take all <code>PublishedResources</code> and bundle them into the pre-existing    <code>APIExport</code> in the org workspace. This APIExport can then be bound in the org workspace itself    (or later any workspaces (depending on permissions)) and be used there.</li> <li>kcp automatically provides a virtual workspace for the <code>APIExport</code> and this is what the Sync Agent    then uses to watch all objects for the relevant resources in kcp (i.e. in all workspaces).</li> <li>The Sync Agent will now begin to synchronize objects back and forth between the service cluster    and kcp.</li> </ol>"},{"location":"#details","title":"Details","text":""},{"location":"#data-flow-direction","title":"Data Flow Direction","text":"<p>It might be a bit confusing at first: The <code>PublishedResource</code> CRD describes the world from the standpoint of a service owner, i.e. a person or team that owns a Kubernetes cluster and is tasked with making their CRDs available in kcp (i.e. \"publish\" them).</p> <p>However the actual data flow later will work in the opposite direction: users creating objects inside their kcp workspaces serve as the source of truth. From there they are synced down to the service cluster, which is doing the projection of the <code>PublishedResource</code> in reverse.</p> <p>Of course additional, auxiliary (related) objects could originate on the service cluster. For example if you create a Certificate object in a kcp workspace and it's synced down, cert-manager will then acquire the certificate and create a Kubernetes <code>Secret</code>, which will have to be synced back up (into a kcp workspace, where the certificate originated from). So the source of truth can also be, for auxiliary resources, on the service cluster.</p>"},{"location":"#sync-agent-naming","title":"Sync Agent Naming","text":"<p>Each Sync Agent must have a name, like \"nora\" or \"oskar\". The FQ name for a Sync Agent is <code>&lt;agentname&gt;.&lt;apigroup&gt;</code>, so if the user in kcp had created a new <code>APIExport</code> named <code>databases.examplecorp</code>, the name of the Sync Agent that serves this Service (sic) could be <code>nora.databases.examplecorp</code>.</p>"},{"location":"#uniqueness","title":"Uniqueness","text":"<p>A single <code>APIExport</code> in kcp must only be processed by exactly 1 Sync Agent. There is currently no mechanism planned to subdivide an <code>APIExport</code> into shards, where multiple service clusters (and therefore multiple Sync Agents) could process each shard.</p> <p>Later the Sync Agent might be extended with Label Selectors, alternatively they might also \"claim\" any object by annotating it in the kcp workspace. These things are not yet worked out, so for now we have this 1:1 restriction.</p> <p>Sync Agents make use of leader election, so it's perfectly fine to have multiple Sync Agent replicas, as long as only one them is leader and actually doing work.</p>"},{"location":"#kcp-awareness","title":"kcp-awareness","text":"<p>controller-runtime can be used in a \"kcp-aware\" mode, where the cache, clients, mappers etc. are aware of the workspace information. This however is neither well tested upstream and the code would require shard-admin permissions to behave like this work regular kcp workspaces. The controller-runtime fork's kcp-awareness is really more geared towards working in virtual workspaces.</p> <p>Because of this the Sync Agent needs to get a kubeconfig to kcp that already points to the <code>APIExport</code>'s workspace (i.e. the <code>server</code> URL already contains a <code>/clusters/root:myorg</code> path). The basic controllers in the Sync Agent then treat this as a plain ol', regular Kubernetes cluster (no kcp-awareness).</p> <p>To this end, the Sync Agent will, upon startup, try to access the <code>cluster</code> object in the target workspace. This is to resolve the cluster name (e.g. <code>root:myorg</code>) into a logicalcluster name (e.g. <code>gibd3r1sh</code>). The Sync Agent has to know which logicalcluster the target workspace represents in order to query resources properly.</p> <p>Only the controllers that are later responsible for interacting with the virtual workspace are kcp-aware. They have to be in order to know what workspace a resource is living in.</p>"},{"location":"#publishedresources","title":"PublishedResources","text":"<p>A <code>PublishedResource</code> describes which CRD should be made available inside kcp. The CRD name can be projected (i.e. renamed), so a <code>kubermatic.k8c.io/v1 Cluster</code> can become a <code>cloud.examplecorp/v1 KubernetesCluster</code>.</p> <p>In addition to projecting (mapping) the GVK, the <code>PublishedResource</code> also contains optional naming rules, which influence how the local objects that the Sync Agent is creating are named.</p> <p>As a single Sync Agent serves a single service, the API group used in kcp is the same for all <code>PublishedResources</code>. It's the API group configured in the <code>APIExport</code> inside kcp (created in step 1 in the overview above).</p> <p>To prevent chaos, <code>PublishedResources</code> are immutable: handling the case that a PR first wants to publish <code>kubermatic.k8c.io/v1 Cluster</code> and then suddenly <code>kubermatic.k8c.io/v1 User</code> resources would mean to re-sync and cleanup everything in all affected kcp workspaces. The Sync Agent would need to be able to delete and recreate objects to follow this GVK change, which is a level of complexity we simply do not want to deal with at this point in time. Also, <code>APIResourceSchemas</code> are immutable themselves.</p> <p>More information is available in the Publishing Resources guide.</p>"},{"location":"#apiexports","title":"APIExports","text":"<p>An <code>APIExport</code> in kcp combines multiple <code>APIResourceSchemas</code> (ARS). Each ARS is created based on a <code>PublishedResource</code> in the service cluster.</p> <p>To prevent data loss, ARS are never removed from an <code>APIExport</code>. We simply do not have enough experience to really know what happens when an ARS would suddenly become unavailable. To prevent damage and confusion, the Sync Agent will only ever add new ARS to the one <code>APIExport</code> it manages.</p>"},{"location":"#controllers","title":"Controllers","text":"<p>The Sync Agent consists of a number of independent controllers.</p>"},{"location":"#apiexport","title":"apiexport","text":"<p>This controller aggregates the <code>PublishedResources</code> and manages a single <code>APIExport</code> in kcp.</p>"},{"location":"#apiresourceschema","title":"apiresourceschema","text":"<p>This controller takes <code>PublishedResources</code>, projects and converts them and creates <code>APIResourceSchemas</code> in kcp.</p>"},{"location":"#syncmanager","title":"syncmanager","text":"<p>This controller watches the <code>APIExport</code> and waits for the virtual workspace to become available. It also watches all <code>PublishedResources</code> (PRs) and reconciles when any of them is changed (they are immutable, but the controller is still reacting to any events on them).</p> <p>The controller will then setup a controller-runtime <code>Cluster</code> abstraction for the virtual workspace and then start many <code>sync</code> controllers (one for each <code>PublishedResource</code>). Whenever PRs change, the syncmanager will make sure that the correct set of <code>sync</code> controller is running.</p>"},{"location":"#sync","title":"sync","text":"<p>This is where the meat and potatoes happen. The sync controller is started for a single <code>PublishedResource</code> and is responsible for synchronizing all objects for that resource between the local service cluster and kcp.</p> <p>The <code>sync</code> controller was written to handle a single <code>PublishedResource</code> so that it does not have to deal with dynamically registering/stopping watches on its own. Instead the sync controller can be written as more or less \"normal\" controller-runtime controller.</p>"},{"location":"consuming-services/","title":"Consuming Services","text":"<p>This document describes how to use (consume) services offered by a Sync Agent.</p>"},{"location":"consuming-services/#background","title":"Background","text":"<p>A \"service\" defines a unique Kubernetes API Group and offers a number of resources (types) to use. A service could offer certificate management, databases, cloud infrastructure or any other set of Kubernetes resources.</p> <p>Services are provided by service owners, who run their own Kubernetes clusters and take care of the maintenance and scaling tasks for the workload provisioned by all users of the service(s) they offer.</p> <p>A Service provided by a Sync Agent should not be confused with a Kubernetes Service. Internally, a \"Sync Agent Service\" is ultimately translated into a kcp <code>APIExport</code> with a number of <code>APIResourceSchemas</code> (which are more or less equivalent to CRDs).</p>"},{"location":"consuming-services/#consuming-a-service","title":"Consuming a Service","text":"<p>To consume a service (or to make use of an <code>APIExport</code>) you have to create an <code>APIBinding</code> object in the kcp workspace where the service should be used. This section assumes that you are familiar with kcp on the command line and have the kcp kubectl plugin installed.</p> <p>First you need to get the kubeconfig for accessing your kcp workspaces. Once you have set your kubeconfig up, make sure you're in the correct namespace by using <code>kubectl ws &lt;path to your workspace&gt;</code>. Use <code>kubectl ws .</code> if you're unsure where you're at.</p> <p>To enable a Service, use <code>kcp bind apiexport</code> and specify the path to and name of the <code>APIExport</code>.</p> <pre><code># kubectl kcp bind apiexport &lt;path to APIExport&gt;:&lt;API Group of the Service&gt;\nkubectl kcp bind apiexport :root:my-org:my.fancy.api\n</code></pre> <p>Without the plugin, you can create an <code>APIBinding</code> manually, simply <code>kubectl apply</code> this:</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: my.fancy.api\nspec:\n  reference:\n    export:\n      name: my.fancy.api\n      path: root:my-org\n</code></pre> <p>Shortly after, the new API will be available in the workspace. Check via <code>kubectl api-resources</code>. You can now create objects for types in that API group to your liking and they will be synced and processed behind the scenes.</p> <p>Note that a Service often has related resources, often Secrets and ConfigMaps. You must explicitly allow the Service to access these in your workspace and this means editing/patching the <code>APIBinding</code> object (the kcp kubectl plugin currently has no support for managing permission claims). For each of the claimed resources, you have to accept or reject them:</p> <pre><code>spec:\n  permissionClaims:\n    # Nearly all Sync Agents require access to namespaces, rejecting this will\n    # most likely break the Service, even more than rejecting any other claim.\n    - all: true\n      resources: namespaces\n      state: Accepted\n    - all: true\n      resources: secrets\n      state: Accepted # or Rejected\n</code></pre> <p>Rejecting a claim will severely impact a Service, if not even break it. Consult with the Service's documentation or the service owner if rejecting a claim is supported.</p> <p>When you change into (<code>kubctl ws \u2026</code>) a different workspace, kubectl will inform you if there are outstanding permission claims that you need to accept or reject.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#can-i-run-multiple-sync-agents-on-the-same-service-cluster","title":"Can I run multiple Sync Agents on the same service cluster?","text":"<p>Yes, absolutely, however you must configure them properly:</p> <p>A given <code>PublishedResource</code> must only ever be processed by a single Sync Agent Pod. The Helm chart configures leader-election by default, so you can scale up to have Pods on stand-by if needed.</p> <p>By default the Sync Agent will discover and process all <code>PublishedResources</code> in your cluster. Use the <code>--published-resource-selector</code> (<code>publishedResourceSelector</code> in the Helm values.yaml) to restrict an Agent to a subset of published resources.</p>"},{"location":"faq/#can-i-synchronize-multiple-kcp-setups-onto-the-same-service-cluster","title":"Can I synchronize multiple kcp setups onto the same service cluster?","text":"<p>Only if you have distinct API groups (and therefore also distinct <code>PublishedResources</code>) for them. You cannot currently publish the same API group onto multiple kcp setups. See issue #13 for more information.</p>"},{"location":"faq/#what-happens-when-crds-are-updated","title":"What happens when CRDs are updated?","text":"<p>At the moment, nothing. <code>APIResourceSchemas</code> in kcp are immutable and the Sync Agent currently does not attempt to update existing schemas in an <code>APIExport</code>. If you add a new CRD that you want to publish, that's fine, it will be added to the <code>APIExport</code>. But changes to existing CRDs require manual work.</p> <p>To trigger an update:</p> <ul> <li>remove the <code>APIResourceSchema</code> from the <code>latestResourceSchemas</code>,</li> <li>delete the <code>APIResourceSchema</code> object in kcp,</li> <li>restart the api-syncagent</li> </ul>"},{"location":"faq/#does-the-sync-agent-handle-permission-claims","title":"Does the Sync Agent handle permission claims?","text":"<p>Only those required for its own operation. If you configure a namespaced resource to sync, it will automatically add a claim for <code>namespaces</code> in kcp, plus it will add either <code>configmaps</code> or <code>secrets</code> if related resources are configured in a <code>PublishedResource</code>. But you cannot specify additional permissions claims.</p>"},{"location":"faq/#i-am-seeing-errors-in-the-agent-logs-whats-going-on","title":"I am seeing errors in the agent logs, what's going on?","text":"<p>Errors like</p> <p>reflector.go:561] k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list example.com/v1, Kind=Dummy: the server could not find the requested resource</p> <p>or</p> <p>reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: Failed to watch kcp.example.com/v1, Kind=Dummy: failed to list kcp.example.com/v1, Kind=Dummy: the server could not find the requested resource\" logger=\"UnhandledError\"</p> <p>are typical when bootstrapping new APIExports in kcp. They are only cause for concern if they persist after configuring all PublishedResources.</p>"},{"location":"getting-started/","title":"Getting Started with the Sync Agent","text":"<p>All that is necessary to run the Sync Agent is a running Kubernetes cluster (for testing you can use kind) and a kcp installation.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster to run the Sync Agent in.</li> <li>A running kcp installation as the source of truth.</li> <li>A kubeconfig with admin or comparable permissions in a specific kcp workspace.</li> </ul>"},{"location":"getting-started/#apiexport-setup","title":"APIExport Setup","text":"<p>Before installing the Sync Agent it is necessary to create an <code>APIExport</code> on kcp. The <code>APIExport</code> should be empty, because it is updated later by the Sync Agent, but it defines the new API group we're introducing. An example file could look like this:</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIExport\nmetadata:\n  name: test.example.com\nspec: {}\n</code></pre> <p>Create a file with a similar content (you most likely want to change the name, as that is the API group under which your published resources will be made available) and create it in a kcp workspace of your choice:</p> <pre><code># use the kcp kubeconfig\n$ export KUBECONFIG=/path/to/kcp.kubeconfig\n\n# nativagate to the workspace where the APIExport should exist\n$ kubectl ws :workspace:you:want:to:create:it\n\n# create it\n$ kubectl create --filename apiexport.yaml\napiexport/test.example.com created\n</code></pre>"},{"location":"getting-started/#sync-agent-installation","title":"Sync Agent Installation","text":"<p>The Sync Agent can be installed into any namespace, but in our example we are going with <code>kcp-system</code>. It doesn't necessarily have to live in the same Kubernetes cluster where it is synchronizing data to, but that is the common setup. Ultimately the Sync Agent synchronizes data between two kube endpoints.</p> <p>Now that the <code>APIExport</code> is created, switch to the Kubernetes cluster from which you wish to publish resources. You will need to ensure that a kubeconfig with access to the kcp workspace that the <code>APIExport</code> has been created in is stored as a <code>Secret</code> on this cluster. Make sure that the kubeconfig points to the right workspace (not necessarily the <code>root</code> workspace).</p> <p>This can be done via a command like this:</p> <pre><code>$ kubectl create secret generic kcp-kubeconfig \\\n  --namespace kcp-system \\\n  --from-file \"kubeconfig=admin.kubeconfig\"\n</code></pre>"},{"location":"getting-started/#helm-chart-setup","title":"Helm Chart Setup","text":"<p>The Sync Agent is shipped as a Helm chart and to install it, the next step is preparing a <code>values.yaml</code> file for the Sync Agent Helm chart. We need to pass the target <code>APIExport</code>, a name for the Sync Agent itself and a reference to the kubeconfig secret we just created.</p> <pre><code># Required: the name of the APIExport in kcp that this Sync Agent is supposed to serve.\napiExportName: test.example.com\n\n# Required: This Agent's public name, used to signal ownership over locally synced objects.\n# This value must be a valid Kubernetes label value, see\n# https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set\n# for more information.\n# Changing this value after the fact will make the agent ignore previously created objects,\n# so beware and relabel if necessary.\nagentName: unique-test\n\n# Required: Name of the Kubernetes Secret that contains a \"kubeconfig\" key,\n# with the kubeconfig provided by kcp to access it.\nkcpKubeconfig: kcp-kubeconfig\n</code></pre> <p>Once this <code>values.yaml</code> file is prepared, install a recent development build of the Sync Agent:</p> <pre><code>helm repo add kcp https://kcp-dev.github.io/helm-charts\nhelm repo update\n\nhelm install kcp-api-syncagent kcp/api-syncagent \\\n  --values values.yaml \\\n  --namespace kcp-system\n</code></pre> <p>Two <code>kcp-api-syncagent</code> Pods should start in the <code>kcp-system</code> namespace. If they crash you will need to identify the reason from container logs. A possible issue is that the provided kubeconfig does not have permissions against the target kcp workspace.</p>"},{"location":"getting-started/#service-cluster-rbac","title":"Service Cluster RBAC","text":"<p>The Sync Agent usually requires additional RBAC on the service cluster to function properly. The Helm chart will automatically allow it to read CRDs, namespaces and Secrets, but depending on how you configure your PublishedResources, additional permissions need to be created.</p> <p>For example, if the Sync Agent is meant to create <code>Certificate</code> objects (defined by cert-manager), you would need to grant it permissions on those:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: 'api-syncagent:unique-test'\nrules:\n  - apiGroups:\n      - cert-manager.io\n    resources:\n      - certificates\n    verbs:\n      - get\n      - list\n      - watch\n      - create\n      - update\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: 'api-syncagent:unique-test'\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: 'api-syncagent:unique-test'\nsubjects:\n  - kind: ServiceAccount\n    name: 'kcp-api-syncagent'\n    namespace: kcp-system\n</code></pre> <p>NB: Even though the PublishedResources might only create/update Certificates in a single namespace, due to the inner workings of the Agent they will still be watched (cached) cluster-wide. So you can tighten permissions on <code>create</code>/<code>update</code> operations to certain namespaces, but <code>watch</code> permissions need to be granted cluster-wide.</p>"},{"location":"getting-started/#kcp-rbac","title":"kcp RBAC","text":"<p>The Helm chart is installed on the service cluster and so cannot provision the necessary RBAC for the Sync Agent within kcp. Usually whoever creates the <code>APIExport</code> is also responsible for creating the RBAC rules that grant the Agent access.</p> <p>The Sync Agent needs to</p> <ul> <li>access the workspace of its <code>APIExport</code>,</li> <li>get the <code>LogicalCluster</code>,</li> <li>manage its <code>APIExport</code>,</li> <li>manage <code>APIResourceSchemas</code> and</li> <li>access the virtual workspace for its <code>APIExport</code>.</li> </ul> <p>This can be achieved by applying RBAC like this in the workspace where the <code>APIExport</code> resides:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: api-syncagent-mango\nrules:\n  # get the LogicalCluster\n  - apiGroups:\n      - core.kcp.io\n    resources:\n      - logicalclusters\n    resourceNames:\n      - cluster\n    verbs:\n      - get\n  # manage its APIExport\n  - apiGroups:\n      - apis.kcp.io\n    resources:\n      - apiexports\n    resourceNames:\n      - test.example.com\n    verbs:\n      - get\n      - list\n      - watch\n      - patch\n      - update\n  # manage APIResourceSchemas\n  - apiGroups:\n      - apis.kcp.io\n    resources:\n      - apiresourceschemas\n    verbs:\n      - get\n      - list\n      - watch\n      - create\n  # access the virtual workspace\n  - apiGroups:\n      - apis.kcp.io\n    resources:\n      - apiexports/content\n    resourceNames:\n      - test.example.com\n    verbs:\n      - '*'\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: api-syncagent-mango:system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: api-syncagent-mango\nsubjects:\n  - kind: User\n    name: api-syncagent-mango\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: api-syncagent-mango:access\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kcp:workspace:access\nsubjects:\n  - kind: User\n    name: api-syncagent-mango\n</code></pre>"},{"location":"getting-started/#publish-resources","title":"Publish Resources","text":"<p>Once the Sync Agent Pods are up and running, you should be able to follow the Publishing Resources guide.</p>"},{"location":"getting-started/#consume-service","title":"Consume Service","text":"<p>Once resources have been published through the Sync Agent, they can be consumed on the kcp side (i.e. objects on kcp will be synced back and forth with the service cluster). Follow the guide to consuming services.</p>"},{"location":"publish-resources/","title":"Publishing Resources","text":"<p>The guide describes the process of making a resource (usually defined by a CustomResourceDefinition) of one Kubernetes cluster (the \"service cluster\" or \"local cluster\") available for use in kcp. This involves setting up an <code>APIExport</code> and then installing the Sync Agent and defining <code>PublishedResources</code> in the local cluster.</p> <p>All of the documentation and API types are worded and named from the perspective of a service owner, the person(s) who own a service and want to make it available to consumers in kcp.</p>"},{"location":"publish-resources/#high-level-overview","title":"High-level Overview","text":"<p>A \"service\" comprises a set of resources within a single Kubernetes API group. It doesn't need to be all of the resources in that group, service owners are free and encouraged to only make a subset of resources (i.e. a subset of CRDs) available for use in kcp.</p> <p>For each of the CRDs on the service cluster that should be published, the service owner creates a <code>PublishedResource</code> object, which will contain both which CRD to publish, as well as numerous other important settings that influence the behaviour around handling the CRD.</p> <p>When publishing a resource (CRD), exactly one version is published. All others are ignored from the standpoint of the resource synchronization logic.</p> <p>All published resources together form the APIExport. When a service is enabled in a workspace (i.e. it is bound to it), users can manage objects for the projected resources described by the published resources. These objects will be synced from the workspace onto the service cluster, where they are meant to be processed in whatever way the service owners desire. Any possible status information (in the <code>status</code> subresource) will in turn be synced back up into the workspace where the user can inspect it.</p> <p>Additionally, a published resource can describe additional so-called \"related resources\". These usually originate on the service cluster and could be for example connection detail secrets created by Crossplane, but could also originate in the user workspace and just be additional, auxiliary resources that need to be synced down to the service cluster.</p>"},{"location":"publish-resources/#publishedresource","title":"<code>PublishedResource</code>","text":"<p>In its simplest form (which is rarely practical) a <code>PublishedResource</code> looks like this:</p> <pre><code>apiVersion: syncagent.kcp.io/v1alpha1\nkind: PublishedResource\nmetadata:\n  name: publish-certmanager-certs # name can be freely chosen\nspec:\n  resource:\n    kind: Certificate\n    apiGroup: cert-manager.io\n    version: v1\n</code></pre> <p>However, you will most likely apply more configuration and use features described below.</p>"},{"location":"publish-resources/#filtering","title":"Filtering","text":"<p>The Sync Agent can be instructed to only work on a subset of resources in kcp. This can be restricted by namespace and/or label selector.</p> <pre><code>apiVersion: syncagent.kcp.io/v1alpha1\nkind: PublishedResource\nmetadata:\n  name: publish-certmanager-certs # name can be freely chosen\nspec:\n  resource: ...\n  filter:\n    namespace: my-app\n    resource:\n      matchLabels:\n        foo: bar\n</code></pre>"},{"location":"publish-resources/#schema","title":"Schema","text":"<p>Warning: The actual CRD schema is always copied verbatim. All projections  etc. have to take into account that the resource contents must be expressible without changes to the schema, so you cannot define entirely new fields in an object that are not defined by the original CRD.</p>"},{"location":"publish-resources/#projection","title":"Projection","text":"<p>For stronger separation of concerns and to enable whitelabelling of services, the type meta for can be projected, i.e. changed between the local service cluster and kcp. You could for example rename <code>Certificate</code> from cert-manager to <code>Sertifikat</code> inside kcp.</p> <p>Note that the API group of all published resources is always changed to the one defined in the APIExport object (meaning 1 Sync Agent serves all the selected published resources under the same API group). That is why changing the API group cannot be configured in the projection.</p> <p>Besides renaming the Kind and Version, dependent fields like Plural, ShortNames and Categories can be adjusted to fit the desired naming scheme in kcp. The Plural name is computed automatically, but can be overridden. ShortNames and Categories are copied unless overwritten in the <code>PublishedResource</code>.</p> <p>It is also possible to change the scope of resources, i.e. turning a namespaced resource into a cluster-wide. This should be used carefully and might require extensive mutations.</p> <pre><code>apiVersion: syncagent.kcp.io/v1alpha1\nkind: PublishedResource\nmetadata:\n  name: publish-certmanager-certs # name can be freely chosen\nspec:\n  resource: ...\n  projection:\n    version: v1beta1\n    kind: Sertifikat\n    plural: Sertifikater\n    shortNames: [serts]\n    # categories: [management]\n    # scope: Namespaced # change only when you know what you're doing\n</code></pre> <p>Consumers (end users) in kcp would then ultimately see projected names only. Note that GVK projection applies only to the synced object itself and has no effect on the contents of these objects. To change the contents, use external solutions like Crossplane to transform objects.</p>"},{"location":"publish-resources/#re-naming","title":"(Re-)Naming","text":"<p>Since the Sync Agent ingests resources from many different Kubernetes clusters (workspaces) and combines them onto a single cluster, resources have to be renamed to prevent collisions and also follow the conventions of whatever tooling ultimately processes the resources locally.</p> <p>The renaming is configured in <code>spec.naming</code>. In there, renaming patterns are configured, where pre-defined placeholders can be used, for example <code>foo-$placeholder</code>. The following placeholders are available:</p> <ul> <li><code>$remoteClusterName</code> \u2013 the workspace's cluster name (e.g. \"1084s8ceexsehjm2\")</li> <li><code>$remoteNamespace</code> \u2013 the original namespace used by the consumer inside the workspace</li> <li><code>$remoteNamespaceHash</code> \u2013 first 20 hex characters of the SHA-1 hash of <code>$remoteNamespace</code></li> <li><code>$remoteName</code> \u2013 the original name of the object inside the workspace (rarely used to construct   local namespace names)</li> <li><code>$remoteNameHash</code> \u2013 first 20 hex characters of the SHA-1 hash of <code>$remoteName</code></li> </ul> <p>If nothing is configured, the default ensures that no collisions will happen: Each workspace in kcp will create a namespace on the local cluster, with a combination of namespace and name hashes used for the actual resource names.</p> <pre><code>apiVersion: syncagent.kcp.io/v1alpha1\nkind: PublishedResource\nmetadata:\n  name: publish-certmanager-certs # name can be freely chosen\nspec:\n  resource: ...\n  naming:\n    # This is the implicit default configuration.\n    namespace: \"$remoteClusterName\"\n    name: \"cert-$remoteNamespaceHash-$remoteNameHash\"\n</code></pre>"},{"location":"publish-resources/#mutation","title":"Mutation","text":"<p>Besides projecting the type meta, changes to object contents are also nearly always required. These can be configured in a number of way in the <code>PublishedResource</code>.</p> <p>Configuration happens <code>spec.mutation</code> and there are two fields:</p> <ul> <li><code>spec</code> contains the mutation rules when syncing the desired state (often in <code>spec</code>, but can also   be other top-level fields) from the remote side to the local side. Use this to apply defaulting,   normalising, and enforcing rules.</li> <li><code>status</code> contains the mutation rules when syncing the <code>status</code> subresource back from the local   cluster up into kcp. Use this to normalize names and values (e.g. if you rewrote   <code>.spec.secretName</code> from <code>\"foo\"</code> to <code>\"dfkbssbfh\"</code>, make sure the status does not \"leak\" this name   by accident).</li> </ul> <p>Mutation is always done as a series of steps. Each step does exactly one thing and only one must be configured per step.</p> <pre><code>apiVersion: syncagent.kcp.io/v1alpha1\nkind: PublishedResource\nmetadata:\n  name: publish-certmanager-certs # name can be freely chosen\nspec:\n  resource: ...\n  mutation:\n    spec:\n      # choose one per step\n      - regex: ...\n        template: ...\n        delete: ...\n</code></pre>"},{"location":"publish-resources/#regex","title":"Regex","text":"<pre><code>regex:\n  path: \"json.path[expression]\"\n  pattern: \"(.+)\"\n  replacement: \"foo-\\\\1\"\n</code></pre> <p>This mutation applies a regular expression to a single value inside the document. JSON path is the usual path, without a leading dot.</p>"},{"location":"publish-resources/#template","title":"Template","text":"<pre><code>template:\n  path: \"json.path[expression]\"\n  template: \"{{ .LocalObject.ObjectMeta.Namespace }}\"\n</code></pre> <p>This mutation applies a Go template expression to a single value inside the document. JSON path is the usual path, without a leading dot.</p>"},{"location":"publish-resources/#delete","title":"Delete","text":"<pre><code>delete:\n  path: \"json.path[expression]\"\n</code></pre> <p>This mutation simply removes the value at the given path from the document. JSON path is the usual path, without a leading dot.</p>"},{"location":"publish-resources/#related-resources","title":"Related Resources","text":"<p>The processing of resources on the service cluster often leads to additional resources being created, like a <code>Secret</code> for each cert-manager <code>Certificate</code> or a connection detail secret created by Crossplane. These need to be made available to the user in their workspaces.</p> <p>Likewise it's possible for auxiliary resources having to be created by the user, for example when the user has to provide credentials.</p> <p>To handle these cases, a <code>PublishedResource</code> can define multiple \"related resources\". Each related resource represents usually one, but can be multiple objects to synchronize between user workspace and service cluster. While the main published resource sync is always workspace-&gt;service cluster, related resources can originate on either side and so either can work as the source of truth.</p> <p>At the moment, only <code>ConfigMaps</code> and <code>Secrets</code> are allowed related resource kinds.</p> <p>For each related resource, the Sync Agent needs to be told how to find the object on the origin side and where to create it on the destination side. There are multiple options that you can choose from.</p> <p>By default all related objects live in the same namespace as the primary object (their owner/parent). If the primary object is cluster scoped, admins must configure additional rules to specify what namespace the ConfigMap/Secret shall be read from and created in.</p> <p>Related resources are always optional. Even if references (see below) are used and their path expression points to a non-existing field in the primary object (e.g. <code>spec.secretName</code> is configured, but that field does not exist in Certificate object), this will simply be treated as \"not yet existing\" and not create an error.</p>"},{"location":"publish-resources/#references","title":"References","text":"<p>A reference is a JSONPath-like expression that are evaluated on both sides of the synchronization. You configure a single path expression (like <code>spec.secretName</code>) and the sync agent will evaluate it in the original primary object (in kcp) and again in the copied primary object (on the service cluster). Since the primary object has already been mutated, the <code>spec.secretName</code> is already rewritten/adjusted to work on the service cluster (for example it was changed from <code>my-secret</code> to <code>jk23h4wz47329rz2r72r92-secret</code> on the service cluster side). By doing it this way, admins only have to think about mutations and rewrites once (when configuring the primary object in the PublishedResource) and the path will yield 2 ready to use values (<code>my-secret</code> and the computed value).</p> <p>The value selected by the path expression must be a string (or number, but it will be coalesced into a string) and can then be further adjusted by applying a regular expression to it.</p> <p>References can only ever select 1 related object. Their upside is that they are simple to understand and easy to use, but require a \"link\" in the primary object that would point to the related object.</p> <p>Here's an example on how to use references to locate the related object.</p> <pre><code>apiVersion: syncagent.kcp.io/v1alpha1\nkind: PublishedResource\nmetadata:\n  name: publish-certmanager-certs\nspec:\n  resource:\n    kind: Certificate\n    apiGroup: cert-manager.io\n    version: v1\n\n  naming:\n    # this is where our CA and Issuer live in this example\n    namespace: kube-system\n    # need to adjust it to prevent collions (normally clustername is the namespace)\n    name: \"$remoteClusterName-$remoteNamespaceHash-$remoteNameHash\"\n\n  related:\n    - # unique name for this related resource. The name must be unique within\n      # one PublishedResource and is the key by which consumers (end users)\n      # can identify and consume the related resource. Common names are\n      # \"connection-details\" or \"credentials\".\n      identifier: tls-secret\n\n      # \"service\" or \"kcp\"\n      origin: service\n\n      # for now, only \"Secret\" and \"ConfigMap\" are supported;\n      # there is no GVK projection for related resources\n      kind: Secret\n\n      # configure where in the parent object we can find the child object\n      object:\n        # Object can use either reference, labelSelector or expressions. In this\n        # example we use references.\n        reference:\n          # This path is evaluated in both the local and remote objects, to figure out\n          # the local and remote names for the related object. This saves us from having\n          # to remember mutated fields before their mutation (similar to the last-known\n          # annotation).\n          path: spec.secretName\n\n        # namespace part is optional; if not configured,\n        # Sync Agent assumes the same namespace as the owning resource\n        # namespace:\n        #   reference:\n        #     path: spec.secretName\n        #     regex:\n        #       pattern: '...'\n        #       replacement: '...'\n</code></pre>"},{"location":"publish-resources/#label-selectors","title":"Label Selectors","text":"<p>In some cases, the primary object does not have a link to its child/children objects. In these cases, a label selector can be used. This allows to configure the labels that any related object must have to be included.</p> <p>Notably, this allows for multiple objects that are synced for a single configured related resource. The sync agent will not prevent misconfigurations, so great care must be taken when configuring selectors to not accidentally include too many objects.</p> <p>Additionally, it is assumed that</p> <ul> <li>Primary objects synced from kcp to a service cluster will be renamed, to prevent naming collisions.</li> <li>The renamed objects on the service cluster might contain private, sensitive information that should   not be leaked into kcp workspaces.</li> <li>When there is no explicit name being requested (like by setting <code>spec.secretName</code>), it can be   assumed that the operator on the service cluster that is actually processing the primary object   will use the primary object's name (at least in parts) to construct the names of related objects,   for example a Certificate <code>yaddasupersecretyadda</code> might automatically get a Secret created named   <code>yaddasupersecretyadda-secret</code>.</li> </ul> <p>Since the name of the related object must not leak into a kcp workspace, admins who configure a label selector also always have to provide a naming scheme for the copies of the related objects on the destination side.</p> <p>Namespaces work the same as with references, i.e. by default the same namespace as the primary object is assumed. However you can actually also use label selectors to find the origin namespaces dynamically. So you can configure two label selectors, and then agent will first use the namespace selector to find all applicable namespaces, and then use the other label selector in each of the applicable namespaces to finally locate the related objects. How useful this is depends a lot on how crazy the underlying operators on the service clusters are.</p> <p>Here is an example on how to use label selectors:</p> <pre><code>apiVersion: syncagent.kcp.io/v1alpha1\nkind: PublishedResource\nmetadata:\n  name: publish-certmanager-certs\nspec:\n  resource:\n    kind: Certificate\n    apiGroup: cert-manager.io\n    version: v1\n\n  naming:\n    namespace: kube-system\n    name: \"$remoteClusterName-$remoteNamespaceHash-$remoteNameHash\"\n\n  related:\n    - identifier: tls-secrets\n\n      # \"service\" or \"kcp\"\n      origin: service\n\n      # for now, only \"Secret\" and \"ConfigMap\" are supported;\n      # there is no GVK projection for related resources\n      kind: Secret\n\n      # configure where in the parent object we can find the child object\n      object:\n        # A selector is a standard Kubernetes label selector, supporting\n        # matchLabels and matchExpressions.\n        selector:\n          matchLabels:\n            my-key: my-value\n            another: pair\n\n          # You also need to provide rules on how objects found by this selector\n          # should be named on the destination side of the sync.\n          # Rewrites are either using regular expressions or templated strings,\n          # never both.\n          # The rewrite config is applied to each individual found object.\n          rewrite:\n            regex:\n              pattern: \"foo-(.+)\"\n              replacement: \"bar-\\\\1\"\n\n            # or\n\n            template:\n              template: \"{{ .Name }}-foo\"\n\n\n        # Like with references, the namespace can (or must) be configured explicitly.\n        # You do not need to also use label selectors here, you can mix and match\n        # freely.\n        # namespace:\n        #   reference:\n        #     path: metadata.namespace\n        #     regex:\n        #       pattern: '...'\n        #       replacement: '...'\n</code></pre>"},{"location":"publish-resources/#templates","title":"Templates","text":"<p>The third option to configure how to find/create related objects are templates. These are simple Go template strings (like <code>{{ .Variable }}</code>) that allow to easily configure static values with a sprinkling of dynamic values.</p> <p>This feature has not been fully implemented yet.</p>"},{"location":"publish-resources/#examples","title":"Examples","text":""},{"location":"publish-resources/#provide-certificates","title":"Provide Certificates","text":"<p>This combination of <code>APIExport</code> and <code>PublishedResource</code> make cert-manager certificates available in kcp. The <code>APIExport</code> needs to be created in a workspace, most likely in an organization workspace. The <code>PublishedResource</code> is created wherever the Sync Agent and cert-manager are running.</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIExport\nmetadata:\n  name: certificates.example.corp\nspec: {}\n</code></pre> <pre><code>apiVersion: syncagent.kcp.io/v1alpha1\nkind: PublishedResource\nmetadata:\n  name: publish-certmanager-certs\nspec:\n  resource:\n    kind: Certificate\n    apiGroup: cert-manager.io\n    version: v1\n\n  naming:\n    # this is where our CA and Issuer live in this example\n    namespace: kube-system\n    # need to adjust it to prevent collions (normally clustername is the namespace)\n    name: \"$remoteClusterName-$remoteNamespaceHash-$remoteNameHash\"\n\n  related:\n    - origin: service # service or kcp\n      kind: Secret # for now, only \"Secret\" and \"ConfigMap\" are supported;\n                   # there is no GVK projection for related resources\n\n      # configure where in the parent object we can find\n      # the name/namespace of the related resource (the child)\n      reference:\n        name:\n          # This path is evaluated in both the local and remote objects, to figure out\n          # the local and remote names for the related object. This saves us from having\n          # to remember mutated fields before their mutation (similar to the last-known\n          # annotation).\n          path: spec.secretName\n        # namespace part is optional; if not configured,\n        # Sync Agent assumes the same namespace as the owning resource\n        # namespace:\n        #   path: spec.secretName\n        #   regex:\n        #     pattern: '...'\n        #     replacement: '...'\n</code></pre>"},{"location":"publish-resources/#technical-details","title":"Technical Details","text":"<p>The following sections go into more details of the behind the scenes magic.</p>"},{"location":"publish-resources/#synchronization","title":"Synchronization","text":"<p>Even though the whole configuration is written from the standpoint of the service owner, the actual synchronization logic considers the kcp side as the canonical source of truth. The Sync Agent continuously tries to make the local objects look like the ones in kcp, while pushing status updates back into kcp (if the given <code>PublishedResource</code> (i.e. CRD) has a <code>status</code> subresource enabled).</p>"},{"location":"publish-resources/#local-remote-connection","title":"Local &lt;-&gt; Remote Connection","text":"<p>The Sync Agent tries to keep sync-related metadata on the service cluster, away from the consumers. This is both to prevent vandalism and to hide implementation details.</p> <p>To ensure stability against future changes, once the Sync Agent has determined how a local object should be named, it will remember this decision in the object's metadata. This is so that on future reconciliations, the (potentially costly, but probably not) renaming logic does not need to be applied again. This allows the Sync Agent to change defaults and also allows the service owner to make changes to the naming rules without breaking existing objects.</p> <p>Since we do not want to store metadata on the kcp side, we instead rely on label selectors on the local objects. Each object on the service cluster has a label for the remote cluster name, namespace and object name, and when trying to find the matching local object, the Sync Agent simply does a label-based search.</p> <p>There is currently no sync-related metadata available on source objects (in kcp workspaces), as this would either be annotations (untyped strings...) or require schema changes to allow additional fields in basically random CRDs.</p> <p>Note that fields like <code>generation</code> or <code>resourceVersion</code> are not relevant for any of the sync logic.</p>"},{"location":"publish-resources/#reconcile-loop","title":"Reconcile Loop","text":"<p>The sync loop can be divided into 5 parts:</p> <ol> <li>find the local object</li> <li>handle deletion</li> <li>ensure the destination object exists</li> <li>ensure the destination object's content matches the source object</li> <li>synchronize related resources the same way (repeat 1-4 for each related resource)</li> </ol>"},{"location":"publish-resources/#phase-1-find-the-local-object","title":"Phase 1: Find the Local Object","text":"<p>For this, as mentioned in the connection chapter above, the Sync Agent tries to follow label selectors on the service cluster. This helps prevent cluttering with consumer workspaces with sync metadata. If no object is found to match the labels, that's fine and the loop will continue with phase 2, in which a possible Conflict error (if labels broke) is handled gracefully.</p> <p>The remote object in the workspace becomes the <code>source object</code> and its local equivalent on the service cluster is called the <code>destination object</code>.</p>"},{"location":"publish-resources/#phase-2-handle-deletion","title":"Phase 2: Handle Deletion","text":"<p>A finalizer is used in the kcp workspaces to prevent orphans in the service cluster side. This is the only real evidence in the kcp side that the Sync Agent is even doing things. When a remote (source) object is deleted, the corresponding local object is deleted as well. Once the local object is gone, the finalizer is removed from the source object.</p>"},{"location":"publish-resources/#phase-3-ensure-object-existence","title":"Phase 3: Ensure Object Existence","text":"<p>We have a source object and now need to create the destination. This chart shows what's happening.</p> <pre><code>graph TB\n    A(source object):::state --&gt; B([cleanup if in deletion]):::step\n    B --&gt; C([ensure finalizer on source object]):::step\n    C --&gt; D{exists local object?}\n\n    D -- yes --&gt; I(\"continue with next phase\u2026\"):::state\n    D -- no --&gt; E([apply projection]):::step\n\n    subgraph \"ensure dest object exists\"\n    E --&gt; G([ensure resulting namespace exists]):::step\n    G --&gt; H([create local object]):::step\n    H --&gt; H_err{Errors?}\n    H_err -- Conflict --&gt; J([attempt to adopt existing object]):::step\n    end\n\n    H_err -- success --&gt; I\n    J --&gt; I\n\n    classDef step color:#77F\n    classDef state color:#F77</code></pre> <p>After we followed through with these steps, both the source and destination objects exists and we can continue with phase 4.</p> <p>Resource adoption happens when creation of the initial local object fails. This can happen when labels get mangled. If such a conflict happens, the Sync Agent will \"adopt\" the existing local object by adding / fixing the labels on it, so that for the next reconciliation it will be found and updated.</p>"},{"location":"publish-resources/#phase-4-content-synchronization","title":"Phase 4: Content Synchronization","text":"<p>Content synchronization is rather simple, really.</p> <p>First the source \"spec\" is used to patch the local object. Note that this step is called \"spec\", but should actually be called \"all top-level elements besides <code>apiVersion</code>, <code>kind</code>, <code>status</code> and <code>metadata</code>, but still including some labels and annotations\"; so if you were to publish RBAC objects, the syncer would include <code>roleRef</code> field, for example).</p> <p>To allow proper patch generation, the last known state is kept on the local object, similar to how <code>kubectl</code> creates an annotation for it. This is required for the Sync Agent to properly detect changes made by mutation webhooks on the service cluster.</p> <p>If the published resource (CRD) has a <code>status</code> subresource enabled (not just a <code>status</code> field in its scheme, it must be a real subresource), then the Sync Agent will copy the status from the local object back up to the remote (source) object.</p>"},{"location":"publish-resources/#phase-5-sync-related-resources","title":"Phase 5: Sync Related Resources","text":"<p>The same logic for synchronizing the main published resource applies to their related resources as well. The only difference is that the source side can be either remote (workspace) or local (service cluster).</p> <p>Since the Sync Agent tries its best to keep sync-related data out of kcp workspaces, the last known state for related resources is not kept together with the destination object in the kcp workspaces. Instead all known states (from the main object and all related resources) is kept in a single Secret on the service cluster side.</p>"}]}